{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Azure Machine Learning data collector to log various metrics\n",
    "from azureml.logging import get_azureml_logger\n",
    "logger = get_azureml_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Azure Machine Learning history magic to control history collection\n",
    "# History is off by default, options are \"on\", \"off\", or \"show\"\n",
    "# %azureml history on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, os.path, glob\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.applications import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_dnn():\n",
    "    model = InceptionV3(weights=\"imagenet\", include_top=False) #include_top=False excludes final FC layer\n",
    "    return model\n",
    "\n",
    "def setup_transfer_learninig(model, base_model):\n",
    "    \"\"\"Freeze all layers and compile the model\"\"\"\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False  \n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "def setup_new_classifier(base_model, nb_classes):\n",
    "    \"\"\"Add last layer to the convnet\n",
    "        Args:\n",
    "            base_model: keras model excluding top\n",
    "            nb_classes: # of classes\n",
    "        Returns:\n",
    "            new keras model with last layer\n",
    "    \"\"\"\n",
    "    FC_SIZE = 1024\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(FC_SIZE, activation='relu')(x) #new FC layer, random init\n",
    "    predictions = Dense(nb_classes, activation='softmax')(x) #new softmax layer\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "def setup_finetune(model):\n",
    "    \"\"\"Freeze the bottom NB_IV3_LAYERS and retrain the remaining top layers.\n",
    "        note: NB_IV3_LAYERS corresponds to the top 2 inception blocks in the inceptionv3 arch\n",
    "        Args:\n",
    "            model: keras model\n",
    "    \"\"\"\n",
    "    NB_IV3_LAYERS_TO_FREEZE = 172\n",
    "    \n",
    "    for layer in model.layers[:NB_IV3_LAYERS_TO_FREEZE]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[NB_IV3_LAYERS_TO_FREEZE:]:\n",
    "        layer.trainable = True\n",
    "    model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "def save_dnn(model, folder, filename):\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    sess = K.get_session()\n",
    "    tf.train.write_graph(sess.graph.as_graph_def(), folder, filename, as_text=False)\n",
    "    tf.train.export_meta_graph()\n",
    "    print('saved the graph definition in tensorflow format at: ', filepath)\n",
    "\n",
    "def utils_files_count(directory):\n",
    "    \"\"\"Get number of files by searching directory recursively\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        return 0\n",
    "    cnt = 0\n",
    "    for r, dirs, files in os.walk(directory):\n",
    "        for dr in dirs:\n",
    "            cnt += len(glob.glob(os.path.join(r, dr + \"/*\")))\n",
    "    return cnt\n",
    "\n",
    "def train_generator(folder, batch_size=16, save_to_dir=None):\n",
    "    IM_WIDTH, IM_HEIGHT = 299, 299 #fixed size for InceptionV3\n",
    "    datagen =  ImageDataGenerator(\n",
    "        preprocessing_function=preprocess_input,\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True\n",
    "    )\n",
    "    #generated_folder = os.path.join(folder, '..', 'generated')\n",
    "    if save_to_dir:\n",
    "        if not os.path.exists(save_to_dir):\n",
    "            os.makedirs(save_to_dir)\n",
    "        else:\n",
    "            utils_removeFilesInFolder(save_to_dir)\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        folder,\n",
    "        target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "        batch_size=batch_size,\n",
    "        save_to_dir=save_to_dir\n",
    "    )\n",
    "    return generator\n",
    "\n",
    "def validation_generator(folder, batch_size=16):\n",
    "    IM_WIDTH, IM_HEIGHT = 299, 299 #fixed size for InceptionV3\n",
    "    datagen =  ImageDataGenerator(\n",
    "        preprocessing_function=preprocess_input,\n",
    "        rotation_range=45,\n",
    "        width_shift_range=0,\n",
    "        height_shift_range=0,\n",
    "        shear_range=0,\n",
    "        zoom_range=0,\n",
    "        horizontal_flip=True\n",
    "    )\n",
    "    \n",
    "    generator = datagen.flow_from_directory(\n",
    "        folder,\n",
    "        target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    return generator\n",
    "\n",
    "def utils_removeFilesInFolder(folder):\n",
    "    files = glob.glob(os.path.join(folder,'*'))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "def utils_generated_sample (data_folder, generated_folder, batches_length=1):\n",
    "    generator = train_generator(data_folder, batch_size=16, save_to_dir=generated_folder)\n",
    "    \n",
    "    i = 0\n",
    "    for batch in generator:\n",
    "        i += 1\n",
    "        if i > batches_length:\n",
    "            break  # otherwise the generator would loop indefinitely\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project folder:  D:\\fran\\Work\\eShoConAI\\workbench\n"
     ]
    }
   ],
   "source": [
    "project_folder = %pwd\n",
    "print(\"Project folder: \", project_folder)\n",
    "output_folder = os.path.join(project_folder,'output')\n",
    "data_folder = os.path.join(project_folder, 'data')\n",
    "train_folder = os.path.join(data_folder, 'train')\n",
    "validation_folder = os.path.join(data_folder, 'validation')\n",
    "nb_classes = len(glob.glob(train_folder + \"/*\"))\n",
    "batch_size=16\n",
    "nb_epoch = 32\n",
    "# load weights\n",
    "model_filename = 'model.pb'\n",
    "#if not os.path.exists(output_folder):\n",
    "#    os.makedirs(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 72 images belonging to 4 classes.\n",
      "Found 15 images belonging to 4 classes.\n",
      "Epoch 1/32\n",
      "31/31 [==============================] - 22s 695ms/step - loss: 4.3568 - acc: 0.5787 - val_loss: 2.1624 - val_acc: 0.8667\n",
      "Epoch 2/32\n",
      "31/31 [==============================] - 16s 532ms/step - loss: 0.9435 - acc: 0.8530 - val_loss: 0.5018 - val_acc: 0.8667\n",
      "Epoch 3/32\n",
      "31/31 [==============================] - 16s 529ms/step - loss: 0.2461 - acc: 0.9175 - val_loss: 0.6644 - val_acc: 0.7667\n",
      "Epoch 4/32\n",
      "31/31 [==============================] - 16s 522ms/step - loss: 0.4083 - acc: 0.9176 - val_loss: 0.3239 - val_acc: 0.8778\n",
      "Epoch 5/32\n",
      "31/31 [==============================] - 16s 524ms/step - loss: 0.5309 - acc: 0.8853 - val_loss: 0.3271 - val_acc: 0.8889\n",
      "Epoch 6/32\n",
      "31/31 [==============================] - 16s 529ms/step - loss: 0.3172 - acc: 0.9576 - val_loss: 0.4153 - val_acc: 0.8667\n",
      "Epoch 7/32\n",
      "31/31 [==============================] - 16s 505ms/step - loss: 0.1954 - acc: 0.9295 - val_loss: 0.3072 - val_acc: 0.8667\n",
      "Epoch 8/32\n",
      "31/31 [==============================] - 16s 527ms/step - loss: 0.1178 - acc: 0.9717 - val_loss: 0.2490 - val_acc: 0.9000\n",
      "Epoch 9/32\n",
      "31/31 [==============================] - 16s 512ms/step - loss: 0.1646 - acc: 0.9636 - val_loss: 0.3205 - val_acc: 0.9111\n",
      "Epoch 10/32\n",
      "31/31 [==============================] - 16s 526ms/step - loss: 0.2783 - acc: 0.9519 - val_loss: 0.8607 - val_acc: 0.8667\n",
      "Epoch 11/32\n",
      "31/31 [==============================] - 16s 518ms/step - loss: 0.0295 - acc: 0.9919 - val_loss: 0.2948 - val_acc: 0.9222\n",
      "Epoch 12/32\n",
      "31/31 [==============================] - 16s 503ms/step - loss: 0.0644 - acc: 0.9859 - val_loss: 0.5333 - val_acc: 0.8889\n",
      "Epoch 13/32\n",
      "31/31 [==============================] - 16s 523ms/step - loss: 0.1746 - acc: 0.9656 - val_loss: 0.1799 - val_acc: 0.9556\n",
      "Epoch 14/32\n",
      "31/31 [==============================] - 16s 503ms/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.6356 - val_acc: 0.8667\n",
      "Epoch 15/32\n",
      "31/31 [==============================] - 16s 530ms/step - loss: 0.0473 - acc: 0.9879 - val_loss: 1.1845 - val_acc: 0.8667\n",
      "Epoch 16/32\n",
      "31/31 [==============================] - 16s 508ms/step - loss: 0.3370 - acc: 0.9657 - val_loss: 0.3719 - val_acc: 0.9000\n",
      "Epoch 17/32\n",
      "31/31 [==============================] - 16s 517ms/step - loss: 0.0795 - acc: 0.9757 - val_loss: 0.2994 - val_acc: 0.9222\n",
      "Epoch 18/32\n",
      "31/31 [==============================] - 17s 542ms/step - loss: 0.2436 - acc: 0.9638 - val_loss: 0.9737 - val_acc: 0.8778\n",
      "Epoch 19/32\n",
      "31/31 [==============================] - 17s 535ms/step - loss: 0.0124 - acc: 0.9939 - val_loss: 0.3537 - val_acc: 0.9333\n",
      "Epoch 20/32\n",
      "31/31 [==============================] - 16s 516ms/step - loss: 0.4780 - acc: 0.9356 - val_loss: 0.6933 - val_acc: 0.8556\n",
      "Epoch 21/32\n",
      "31/31 [==============================] - 16s 503ms/step - loss: 0.0170 - acc: 0.9960 - val_loss: 0.5101 - val_acc: 0.9000\n",
      "Epoch 22/32\n",
      "31/31 [==============================] - 16s 513ms/step - loss: 0.3095 - acc: 0.9497 - val_loss: 1.2332 - val_acc: 0.8667\n",
      "Epoch 23/32\n",
      "31/31 [==============================] - 16s 525ms/step - loss: 0.1649 - acc: 0.9717 - val_loss: 0.4694 - val_acc: 0.9333\n",
      "Epoch 24/32\n",
      "31/31 [==============================] - 16s 522ms/step - loss: 0.0697 - acc: 0.9798 - val_loss: 0.4418 - val_acc: 0.9111\n",
      "Epoch 25/32\n",
      "31/31 [==============================] - 16s 507ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.2828 - val_acc: 0.9111\n",
      "Epoch 26/32\n",
      "31/31 [==============================] - 16s 525ms/step - loss: 0.1174 - acc: 0.9676 - val_loss: 0.6789 - val_acc: 0.8778\n",
      "Epoch 27/32\n",
      "31/31 [==============================] - 16s 513ms/step - loss: 5.3334e-04 - acc: 1.0000 - val_loss: 0.5994 - val_acc: 0.9111\n",
      "Epoch 28/32\n",
      "31/31 [==============================] - 16s 508ms/step - loss: 0.2217 - acc: 0.9677 - val_loss: 1.4211 - val_acc: 0.8667\n",
      "Epoch 29/32\n",
      "31/31 [==============================] - 16s 506ms/step - loss: 0.1363 - acc: 0.9779 - val_loss: 0.9136 - val_acc: 0.8889\n",
      "Epoch 30/32\n",
      "31/31 [==============================] - 16s 517ms/step - loss: 0.0190 - acc: 0.9939 - val_loss: 0.4610 - val_acc: 0.9111\n",
      "Epoch 31/32\n",
      "31/31 [==============================] - 16s 514ms/step - loss: 0.1807 - acc: 0.9739 - val_loss: 0.6713 - val_acc: 0.9111\n",
      "Epoch 32/32\n",
      "31/31 [==============================] - 16s 501ms/step - loss: 0.0225 - acc: 0.9919 - val_loss: 1.1681 - val_acc: 0.8778\n"
     ]
    }
   ],
   "source": [
    "# setup model\n",
    "base_model = setup_dnn()\n",
    "model      = setup_new_classifier(base_model, nb_classes)\n",
    "\n",
    "# transfer learning\n",
    "setup_transfer_learninig(model, base_model)\n",
    "\n",
    "history_tl = model.fit_generator(\n",
    "    train_generator(train_folder,batch_size),\n",
    "    steps_per_epoch=500//batch_size, #utils_files_count(train_folder)//batch_size,\n",
    "    epochs=32,\n",
    "    validation_data=validation_generator(validation_folder, batch_size),\n",
    "    validation_steps=100//batch_size, #utils_files_count(validation_folder)//batch_size,\n",
    "    verbose=1,\n",
    "    class_weight='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved the graph definition in tensorflow format at:  D:\\fran\\Work\\eShoConAI\\workbench\\output\\model.pb\n"
     ]
    }
   ],
   "source": [
    "save_dnn(model, output_folder, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils_generated_sample(train_folder, os.path.join(data_folder, 'generated'),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'loss', 'acc', 'val_acc'])\n"
     ]
    }
   ],
   "source": [
    "print(history_tl.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.952715684809\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "print (\"Accuracy: \", numpy.mean(history_tl.history['acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"bracelet\",\"earrings\",\"parasol\",\"thermometer\"]\n",
    "def load_image(img_path):\n",
    "    from keras.preprocessing import image\n",
    "    IM_WIDTH, IM_HEIGHT = 299, 299 #fixed size for InceptionV3\n",
    "    img = image.load_img(img_path, target_size=(IM_WIDTH, IM_HEIGHT))\n",
    "    x = image.img_to_array(img)\n",
    "    x = numpy.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.99994874e-01   5.10283098e-06   1.64173886e-09   1.39166834e-09]]\n",
      "bracelet\n"
     ]
    }
   ],
   "source": [
    "bracelet_path = os.path.join(validation_folder,'bracelet','3506.jpg')\n",
    "bracelet_image = load_image(bracelet_path)\n",
    "preds = model.predict(bracelet_image)\n",
    "print (preds)\n",
    "print (labels[preds.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  8.95389616e-01   3.06148008e-02   1.69649851e-04   7.38259330e-02]]\n",
      "bracelet\n"
     ]
    }
   ],
   "source": [
    "earrings_path = os.path.join(validation_folder,'earrings','3451b.jpg')\n",
    "earrings_image = load_image(earrings_path)\n",
    "preds = model.predict(earrings_image)\n",
    "print (preds)\n",
    "print (labels[preds.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.09924203e-09   2.18095261e-16   1.00000000e+00   3.17252669e-10]]\n",
      "parasol\n"
     ]
    }
   ],
   "source": [
    "parasol_path = os.path.join(validation_folder,'parasol','114a.jpg')\n",
    "parasol_image = load_image(parasol_path)\n",
    "preds = model.predict(parasol_image)\n",
    "print (preds)\n",
    "print (labels[preds.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.25869898e-17   9.73673029e-23   1.04478805e-18   1.00000000e+00]]\n",
      "thermometer\n"
     ]
    }
   ],
   "source": [
    "thermo_path = os.path.join(validation_folder,'thermometer','1866a.jpg')\n",
    "thermo_image = load_image(thermo_path)\n",
    "preds = model.predict(thermo_image)\n",
    "print (preds)\n",
    "print (labels[preds.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dnn2(model, folder, filename):\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    sess = K.get_session()\n",
    "    graph1 = sess.graph.as_graph_def()\n",
    "    vars = {}\n",
    "    for v in tf.trainable_variables():\n",
    "        vars[v.value().name] = sess.run(v)\n",
    "    \n",
    "    with tf.Graph().as_default() as graph2:\n",
    "        consts = {}\n",
    "        for k in vars.keys():\n",
    "            consts[k] = tf.constant(vars[k])\n",
    "        tf.import_graph_def(graph1,input_map={name:consts[name] for name in consts.keys()})\n",
    "        \n",
    "        tf.train.write_graph(sess.graph_def,folder,filename,False)\n",
    "        #tf.train.write_graph(sess.graph.as_graph_def(), folder, filename, as_text=False)\n",
    "        print('saved the graph definition in tensorflow format at: ', filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_dnn2(model, output_folder, 'model2.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_1:0\n",
      "dense_2/Softmax:0\n"
     ]
    }
   ],
   "source": [
    "print (model.input.name)\n",
    "print (model.output.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
