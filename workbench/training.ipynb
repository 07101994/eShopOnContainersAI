{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Azure Machine Learning data collector to log various metrics\n",
    "from azureml.logging import get_azureml_logger\n",
    "logger = get_azureml_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Azure Machine Learning history magic to control history collection\n",
    "# History is off by default, options are \"on\", \"off\", or \"show\"\n",
    "# %azureml history on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CNTK backend\n"
     ]
    }
   ],
   "source": [
    "import sys, os, os.path, glob\n",
    "# To ensure reproducibility, we need to follow these steps\n",
    "# as explained in https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "keras_backend = 'cntk' # valid values: cntk, tensorflow\n",
    "os.environ['KERAS_BACKEND'] = keras_backend\n",
    "\n",
    "seed = 1343\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "import random as rn\n",
    "rn.seed(seed)\n",
    "\n",
    "if keras_backend == 'tensorflow':\n",
    "    import tensorflow as tf\n",
    "\n",
    "    # Force TensorFlow to use single thread.\n",
    "    # Multiple threads are a potential source of\n",
    "    # non-reproducible results.\n",
    "    # For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "    session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_learning_phase(False)\n",
    "\n",
    "if keras_backend == 'tensorflow':\n",
    "    tf.set_random_seed(seed)\n",
    "\n",
    "    sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "    K.set_session(sess)\n",
    "\n",
    "if keras_backend == 'cntk':\n",
    "    import cntk as C\n",
    "    C.device.try_set_default_device(C.device.gpu(0))\n",
    "    \n",
    "from keras.applications import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IM_WIDTH, IM_HEIGHT, IM_CHANNELS = 224, 224, 3 #fixed size for InceptionV3\n",
    "\n",
    "def setup_dnn():\n",
    "    #K.set_image_data_format('channels_last')\n",
    "    model = InceptionV3(weights=\"imagenet\", include_top=False, input_shape=(IM_WIDTH, IM_HEIGHT, IM_CHANNELS)) #include_top=False excludes final FC layer\n",
    "    return model\n",
    "\n",
    "def setup_transfer_learninig(model, base_model):\n",
    "    \"\"\"Freeze all layers and compile the model\"\"\"\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False  \n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "def setup_new_classifier(base_model, nb_classes):\n",
    "    \"\"\"Add last layer to the convnet\n",
    "        Args:\n",
    "            base_model: keras model excluding top\n",
    "            nb_classes: # of classes\n",
    "        Returns:\n",
    "            new keras model with last layer\n",
    "    \"\"\"\n",
    "    FC_SIZE = 1024\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(FC_SIZE, activation='relu')(x) #new FC layer, random init\n",
    "    predictions = Dense(nb_classes, activation='softmax')(x) #new softmax layer\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "def setup_finetune(model):\n",
    "    \"\"\"Freeze the bottom NB_IV3_LAYERS and retrain the remaining top layers.\n",
    "        note: NB_IV3_LAYERS corresponds to the top 2 inception blocks in the inceptionv3 arch\n",
    "        Args:\n",
    "            model: keras model\n",
    "    \"\"\"\n",
    "    NB_IV3_LAYERS_TO_FREEZE = 172\n",
    "    \n",
    "    for layer in model.layers[:NB_IV3_LAYERS_TO_FREEZE]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[NB_IV3_LAYERS_TO_FREEZE:]:\n",
    "        layer.trainable = True\n",
    "    model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "def save_dnn(model, folder, filename):\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    sess = K.get_session()\n",
    "    \n",
    "    outputs = [\"input_1\", \"dense_2/Softmax\"]\n",
    "    constant_graph = tf.graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(), outputs)\n",
    "    tf.train.write_graph(constant_graph,folder,filename,as_text=False)\n",
    "    #tf.train.write_graph(sess.graph.as_graph_def(), folder, filename, as_text=False)\n",
    "    #tf.train.export_meta_graph()\n",
    "    print('saved the graph definition in tensorflow format at: ', filepath)\n",
    "\n",
    "def utils_files_count(directory):\n",
    "    \"\"\"Get number of files by searching directory recursively\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        return 0\n",
    "    cnt = 0\n",
    "    for r, dirs, files in os.walk(directory):\n",
    "        for dr in dirs:\n",
    "            cnt += len(glob.glob(os.path.join(r, dr + \"/*\")))\n",
    "    return cnt\n",
    "\n",
    "def train_generator(folder, batch_size=16, save_to_dir=None):\n",
    "    datagen =  ImageDataGenerator(\n",
    "        preprocessing_function=preprocess_input,\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True\n",
    "    )\n",
    "    #generated_folder = os.path.join(folder, '..', 'generated')\n",
    "    if save_to_dir:\n",
    "        if not os.path.exists(save_to_dir):\n",
    "            os.makedirs(save_to_dir)\n",
    "        else:\n",
    "            utils_removeFilesInFolder(save_to_dir)\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        folder,\n",
    "        target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "        batch_size=batch_size,\n",
    "        save_to_dir=save_to_dir\n",
    "    )\n",
    "    return generator\n",
    "\n",
    "def validation_generator(folder, batch_size=16):\n",
    "    #IM_WIDTH, IM_HEIGHT = 299, 299 #fixed size for InceptionV3\n",
    "    datagen =  ImageDataGenerator(\n",
    "        preprocessing_function=preprocess_input,\n",
    "        rotation_range=45,\n",
    "        width_shift_range=0,\n",
    "        height_shift_range=0,\n",
    "        shear_range=0,\n",
    "        zoom_range=0,\n",
    "        horizontal_flip=True\n",
    "    )\n",
    "    \n",
    "    generator = datagen.flow_from_directory(\n",
    "        folder,\n",
    "        target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    return generator\n",
    "\n",
    "def utils_removeFilesInFolder(folder):\n",
    "    files = glob.glob(os.path.join(folder,'*'))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "def utils_generated_sample (data_folder, generated_folder, batches_length=1):\n",
    "    generator = train_generator(data_folder, batch_size=16, save_to_dir=generated_folder)\n",
    "    \n",
    "    i = 0\n",
    "    for batch in generator:\n",
    "        i += 1\n",
    "        if i > batches_length:\n",
    "            break  # otherwise the generator would loop indefinitely\n",
    "\n",
    "def save_cntk(model, folder, filename):\n",
    "    import cntk as C\n",
    "    C.combine(model.outputs).save(os.path.join(folder, filename))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project folder:  D:\\fran\\Work\\eShoConAI\\workbench\n"
     ]
    }
   ],
   "source": [
    "project_folder = %pwd\n",
    "print(\"Project folder: \", project_folder)\n",
    "output_folder = os.path.join(project_folder,'outputs')\n",
    "data_folder = os.path.join(project_folder, 'data')\n",
    "train_folder = os.path.join(data_folder, 'train')\n",
    "validation_folder = os.path.join(data_folder, 'validation')\n",
    "nb_classes = len(glob.glob(train_folder + \"/*\"))\n",
    "batch_size=16\n",
    "nb_epoch = 32\n",
    "# load weights\n",
    "model_filename = 'model.caret.pb'\n",
    "#if not os.path.exists(output_folder):\n",
    "#    os.makedirs(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\fran\\AppData\\local\\AmlWorkbench\\Python\\lib\\site-packages\\keras\\backend\\cntk_backend.py:2337: UserWarning: CNTK backend warning: CNTK version not detected. Will using CNTK 2.0 GA as default.\n",
      "  'CNTK backend warning: CNTK version not detected. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 72 images belonging to 4 classes.\n",
      "Found 15 images belonging to 4 classes.\n",
      "Epoch 1/32\n",
      "31/31 [==============================] - 58s 2s/step - loss: 3.5321 - acc: 0.6778 - val_loss: 2.9672 - val_acc: 0.7444\n",
      "Epoch 2/32\n",
      "31/31 [==============================] - 42s 1s/step - loss: 0.7410 - acc: 0.8247 - val_loss: 0.3864 - val_acc: 0.8667\n",
      "Epoch 3/32\n",
      "31/31 [==============================] - 43s 1s/step - loss: 0.3852 - acc: 0.8891 - val_loss: 1.0535 - val_acc: 0.8111\n",
      "Epoch 4/32\n",
      "31/31 [==============================] - 44s 1s/step - loss: 0.2642 - acc: 0.9214 - val_loss: 0.4107 - val_acc: 0.9000\n",
      "Epoch 5/32\n",
      "31/31 [==============================] - 40s 1s/step - loss: 0.5472 - acc: 0.9092 - val_loss: 0.5422 - val_acc: 0.8556\n",
      "Epoch 6/32\n",
      "31/31 [==============================] - 41s 1s/step - loss: 0.3123 - acc: 0.9474 - val_loss: 1.3453 - val_acc: 0.8556\n",
      "Epoch 7/32\n",
      "31/31 [==============================] - 40s 1s/step - loss: 0.2112 - acc: 0.9358 - val_loss: 2.3539 - val_acc: 0.8333\n",
      "Epoch 8/32\n",
      "31/31 [==============================] - 41s 1s/step - loss: 0.3045 - acc: 0.9475 - val_loss: 0.7979 - val_acc: 0.8444\n",
      "Epoch 9/32\n",
      "31/31 [==============================] - 40s 1s/step - loss: 0.1957 - acc: 0.9313 - val_loss: 0.8869 - val_acc: 0.8000\n",
      "Epoch 10/32\n",
      "31/31 [==============================] - 43s 1s/step - loss: 0.5833 - acc: 0.9097 - val_loss: 0.4638 - val_acc: 0.8000\n",
      "Epoch 11/32\n",
      "31/31 [==============================] - 41s 1s/step - loss: 0.0735 - acc: 0.9838 - val_loss: 0.6755 - val_acc: 0.8333\n",
      "Epoch 12/32\n",
      "31/31 [==============================] - 41s 1s/step - loss: 0.0556 - acc: 0.9818 - val_loss: 0.6159 - val_acc: 0.8667\n",
      "Epoch 13/32\n",
      "31/31 [==============================] - 42s 1s/step - loss: 0.5349 - acc: 0.9074 - val_loss: 0.6525 - val_acc: 0.8444\n",
      "Epoch 14/32\n",
      "31/31 [==============================] - 42s 1s/step - loss: 0.1214 - acc: 0.9719 - val_loss: 1.0799 - val_acc: 0.8444\n",
      "Epoch 15/32\n",
      "31/31 [==============================] - 43s 1s/step - loss: 0.1169 - acc: 0.9658 - val_loss: 0.9619 - val_acc: 0.8667\n",
      "Epoch 16/32\n",
      "31/31 [==============================] - 42s 1s/step - loss: 0.1765 - acc: 0.9636 - val_loss: 1.5390 - val_acc: 0.8333\n",
      "Epoch 17/32\n",
      "31/31 [==============================] - 43s 1s/step - loss: 0.0995 - acc: 0.9778 - val_loss: 1.1723 - val_acc: 0.8222\n",
      "Epoch 18/32\n",
      "31/31 [==============================] - 45s 1s/step - loss: 0.0408 - acc: 0.9879 - val_loss: 2.0631 - val_acc: 0.8000\n",
      "Epoch 19/32\n",
      "31/31 [==============================] - 43s 1s/step - loss: 0.1270 - acc: 0.9638 - val_loss: 2.1586 - val_acc: 0.8222\n",
      "Epoch 20/32\n",
      "31/31 [==============================] - 42s 1s/step - loss: 0.0795 - acc: 0.9838 - val_loss: 0.9994 - val_acc: 0.9000\n",
      "Epoch 21/32\n",
      "31/31 [==============================] - 45s 1s/step - loss: 0.0640 - acc: 0.9858 - val_loss: 1.5261 - val_acc: 0.8556\n",
      "Epoch 22/32\n",
      "31/31 [==============================] - 42s 1s/step - loss: 0.0537 - acc: 0.9838 - val_loss: 2.3302 - val_acc: 0.8444\n",
      "Epoch 23/32\n",
      "31/31 [==============================] - 41s 1s/step - loss: 0.2089 - acc: 0.9636 - val_loss: 0.9977 - val_acc: 0.8556\n",
      "Epoch 24/32\n",
      "31/31 [==============================] - 42s 1s/step - loss: 0.0794 - acc: 0.9839 - val_loss: 0.8696 - val_acc: 0.9111\n",
      "Epoch 25/32\n",
      "31/31 [==============================] - 41s 1s/step - loss: 0.3895 - acc: 0.9499 - val_loss: 1.0451 - val_acc: 0.8667\n",
      "Epoch 26/32\n",
      "31/31 [==============================] - 42s 1s/step - loss: 0.0061 - acc: 0.9980 - val_loss: 1.0544 - val_acc: 0.8667\n",
      "Epoch 27/32\n",
      "31/31 [==============================] - 41s 1s/step - loss: 0.0923 - acc: 0.9860 - val_loss: 0.9820 - val_acc: 0.9000\n",
      "Epoch 28/32\n",
      "31/31 [==============================] - 41s 1s/step - loss: 0.1799 - acc: 0.9657 - val_loss: 1.8130 - val_acc: 0.8556\n",
      "Epoch 29/32\n",
      "31/31 [==============================] - 42s 1s/step - loss: 0.0514 - acc: 0.9879 - val_loss: 1.0900 - val_acc: 0.8778\n",
      "Epoch 30/32\n",
      "31/31 [==============================] - 42s 1s/step - loss: 0.0868 - acc: 0.9819 - val_loss: 1.6344 - val_acc: 0.8667\n",
      "Epoch 31/32\n",
      "31/31 [==============================] - 42s 1s/step - loss: 0.0259 - acc: 0.9920 - val_loss: 2.7458 - val_acc: 0.6444\n",
      "Epoch 32/32\n",
      "31/31 [==============================] - 41s 1s/step - loss: 0.1305 - acc: 0.9698 - val_loss: 0.8710 - val_acc: 0.8778\n"
     ]
    }
   ],
   "source": [
    "# setup model\n",
    "base_model = setup_dnn()\n",
    "model      = setup_new_classifier(base_model, nb_classes)\n",
    "\n",
    "# transfer learning\n",
    "setup_transfer_learninig(model, base_model)\n",
    "\n",
    "history_tl = model.fit_generator(\n",
    "    train_generator(train_folder,batch_size),\n",
    "    steps_per_epoch=500//batch_size, #utils_files_count(train_folder)//batch_size,\n",
    "    epochs=32,\n",
    "    validation_data=validation_generator(validation_folder, batch_size),\n",
    "    validation_steps=100//batch_size, #utils_files_count(validation_folder)//batch_size,\n",
    "    verbose=1,\n",
    "    class_weight='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#save_dnn(model, output_folder, model_filename)\n",
    "save_cntk(model, output_folder, 'model_cntk.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils_generated_sample(train_folder, os.path.join(data_folder, 'generated'),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history_tl.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "print (\"Accuracy: \", numpy.mean(history_tl.history['acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"bracelet\",\"earrings\",\"parasol\",\"thermometer\"]\n",
    "def load_image(img_path):\n",
    "    from keras.preprocessing import image\n",
    "    #IM_WIDTH, IM_HEIGHT = 299, 299 #fixed size for InceptionV3\n",
    "    img = image.load_img(img_path, target_size=(IM_WIDTH, IM_HEIGHT))\n",
    "    x = image.img_to_array(img)\n",
    "    x = numpy.expand_dims(x, axis=0)\n",
    "    # https://github.com/fchollet/keras/blob/master/keras/applications/imagenet_utils.py\n",
    "    # Image Transformation: channels_last & apply zero-center by mean  pixel: [103.939,116.779,123.68]\n",
    "    x = preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bracelet_path = os.path.join(validation_folder,'bracelet','3506.jpg')\n",
    "bracelet_image = load_image(bracelet_path)\n",
    "preds = model.predict(bracelet_image)\n",
    "print (preds)\n",
    "print (labels[preds.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earrings_path = os.path.join(validation_folder,'earrings','3451b.jpg')\n",
    "earrings_image = load_image(earrings_path)\n",
    "preds = model.predict(earrings_image)\n",
    "print (preds)\n",
    "print (labels[preds.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parasol_path = os.path.join(validation_folder,'parasol','114a.jpg')\n",
    "parasol_image = load_image(parasol_path)\n",
    "preds = model.predict(parasol_image)\n",
    "print (preds)\n",
    "print (labels[preds.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thermo_path = os.path.join(validation_folder,'thermometer','1866a.jpg')\n",
    "thermo_image = load_image(thermo_path)\n",
    "preds = model.predict(thermo_image)\n",
    "print (preds)\n",
    "print (labels[preds.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dnn2(model, folder, filename):\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    sess = K.get_session()\n",
    "    graph1 = sess.graph.as_graph_def()\n",
    "    vars = {}\n",
    "    for v in tf.trainable_variables():\n",
    "        vars[v.value().name] = sess.run(v)\n",
    "    \n",
    "    with tf.Graph().as_default() as graph2:\n",
    "        consts = {}\n",
    "        for k in vars.keys():\n",
    "            consts[k] = tf.constant(vars[k])\n",
    "        tf.import_graph_def(graph1,input_map={name:consts[name] for name in consts.keys()})\n",
    "        \n",
    "        tf.train.write_graph(sess.graph_def,folder,filename,False)\n",
    "        #tf.train.write_graph(sess.graph.as_graph_def(), folder, filename, as_text=False)\n",
    "        print('saved the graph definition in tensorflow format at: ', filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_dnn2(model, output_folder, 'model2.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (model.input.name)\n",
    "print (model.output.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dnn3(model, folder, filename):\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    sess = K.get_session()\n",
    "    graph = sess.graph.as_graph_def()\n",
    "    \n",
    "    outputs = [\"dense_2/Softmax\"]\n",
    "    constant_graph = tf.graph_util.convert_variables_to_constants(sess, graph, outputs)\n",
    "    tf.train.write_graph(constant_graph,folder,filename,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_dnn3(model, output_folder, 'model3.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.get_output_at(\"batch_normalization_1/keras_learning_phase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_dnn(model, output_folder, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thermo_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.image_data_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K.set_image_data_format('channels_first')\n",
    "K.image_data_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cntk as C\n",
    "print(C.device.all_devices())\n",
    "dev_gpu = C.device.all_devices()[0]\n",
    "print(dev_gpu.type)\n",
    "print(C.device.gpu(0))\n",
    "print(C.device.get_gpu_properties(dev_gpu))\n",
    "C.device.try_set_default_device(C.device.gpu(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C.device.try_set_default_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'%.5f' % (-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.api.schema.dataTypes import DataTypes\n",
    "DataTypes.STANDARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
